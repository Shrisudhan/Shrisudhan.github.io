<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shrisudhan Govindarajan</title>
  
  <meta name="author" content="Shrisudhan Govindarajan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

      <!-- Bio -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shrisudhan Govindarajan</name>
              </p>
              <p>
                I am currently a Ph.D. student in <a href="https://www.sfu.ca/computing.html">Computing Science, Simon Fraser University(SFU)</a>, advised by <a herf="https://theialab.ca/">Prof. Andrea Tagliasacchi</a>.
                Before this, I was a Data & Applied Scientist at <a href="https://www.microsoft.com/en-in/msidc">Microsoft India (R&D), Hyderabad</a>. 
                I completed my Dual Degree from <a href="https://www.iitm.ac.in/">IIT Madras</a> with Masters in Data Science.
              </p>
              <p>
                I've had the pleasure of working with <a href="https://www.ee.iitm.ac.in/kmitra/">Prof. Kaushik Mitra</a> from IIT Madras,
                on self-supervised light field synthesis. I've also had the chance to worked with
                <a href="https://www.linkedin.com/in/pawan-baheti-4a67075/?originalSubdomain=in">Pawan Baheti</a> from Qualcomm, India as a 
                part of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2021-india"><strong>Qualcomm Innovation Fellowship, 2021-22</strong></a>.
                
              </p>
              <p>
                My main research interest lies at the intersection of Computer vision and Computer Graphics. 
                I am recently drawn towards the latest research works in NeRF and 3DGS, and their intersection for 3D scene representation.
                I am mainly interested in working towards developing generalizable 3D scene representations.
              </p>
              <!-- <p> -->
                <!-- Link to my <a href="data/DDP_Thesis.pdf">Masters Thesis</a> &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; | &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; -->
                <!-- Link to my <a href="data/BTP_Thesis.pdf">Bachelors Thesis</a> -->  
              <!-- </p> -->

              <p style="text-align:center">
                <a href="mailto:shrisudhan07@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Y9l_dogAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Shrisudhan001">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Shrisudhan">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Shri_photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Shri_photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


      <!-- Research -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody>
      </table>

        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>  <!-- onmouseout="recon_stop()" onmouseover="recon_stopt()" bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/laghash_teaser.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://theialab.github.io/banf/">
                <papertitle>Lagrangian Hashing for Compressed Neural Field Representations                </papertitle>
              </a>
              <br>
              <strong>Shrisudhan Govindarajan*</strong>,
              <a href="https://zenos4mbu.github.io/">Zeno Sambugaro*</a>,
              <a href="https://ahanio.github.io/">Ahan Shabhanov</a>,
              <a href="https://tovacinni.github.io/">Towaki Takikawa</a>,
              <a href="https://wsunid.github.io/">Weiwei Sun</a>,
              <a href="http://drebain.com/">Daniel Rebain</a>,
              <a href="https://webapps.unitn.it/du/it/Persona/PER0003698/Curriculum">Nicola Conci</a>,
              <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a>,
              <a href="https://theialab.ca/">Andrea Tagliasacchi</a>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://theialab.github.io/laghashes/">project</a>
              /
              <a href="">pdf</a>
              /
              <a href="">arXiv</a>
              /
              <a href="">code</a>
              <p>
                A representation for neural fields combining the characteristics of fast training NeRF methods
                that rely on Eulerian grids (i.e.~InstantNGP), with those that employ points equipped with 
                features as a way to represent information (e.g. 3D Gaussian Splatting or PointNeRF).
              </p>
            </td>
          </tr>
        </tbody>
      </table>

        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/banf_teaser.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://theialab.github.io/banf/">
                <papertitle>BANF: Band-limited Neural Fields for Levels of Detail Reconstruction</papertitle>
              </a>
              <br>
              <a href="https://ahanio.github.io/">Ahan Shabhanov</a>,
              <strong>Shrisudhan Govindarajan</strong>,
              <a href="https://codyreading.github.io/">Cody Reading</a>,
              <a href="https://lilygoli.github.io/">Lily Goli</a>,
              <a href="http://drebain.com/">Daniel Rebain</a>,
              <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a>,
              <a href="https://theialab.ca/">Andrea Tagliasacchi</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://theialab.github.io/banf/">project</a>
              /
              <a href="https://arxiv.org/pdf/2404.13024.pdf">pdf</a>
              /
              <a href="https://arxiv.org/abs/2404.13024">arXiv</a>
              /
              <a href="https://github.com/theialab/banf">code</a>
              <p>
                <!-- Effective filtering of neural fields is critical to enable level-of-detail processing in 
                downstream applications, and support operations that involve sampling the field on regular 
                grids (e.g. marching cubes).  -->
                We show that via a simple modification, one can obtain neural fields that are low-pass 
                filtered, and in turn show how this can be exploited to obtain a frequency decomposition of 
                the entire signal.
              </p>
            </td>
          </tr>
        </tbody>
      </table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/dp_teaser.png' width="180" height="90">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Stereo-Knowledge Distillation from dpMV to Dual Pixels for Light Field Video Reconstruction
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=z_VblYsAAAAJ&hl=en">Aryan Garg</a>,
              <a href="https://www.linkedin.com/in/raghav-mallampalli/?originalSubdomain=in">Raghav Mallampalli</a>,
              <a href="https://www.linkedin.com/in/akshat-joshi-324395194/?originalSubdomain=in">Akshat Joshi</a>,
              <strong>Shrisudhan Govindarajan</strong>,
              <a href="https://www.ee.iitm.ac.in/kmitra/research.html">Kaushik Mitra</a>
              <br>
              <em>ICCP</em>, 2024
              <br>
              <a href="">project</a>
              /
              <a href="https://arxiv.org/pdf/2405.11823">pdf</a>
              /
              <a href="https://arxiv.org/abs/2405.11823">arXiv</a>
              /
              <a href="https://github.com/Aryan-Garg/dp-LFVR">code</a>
              <p>
                We collect the first 3-view dual-pixel video dataset, dpMV, and show that dual-pixel methods 
                for light-field reconstruction outperform purely monocular methods, especially in challenging 
                foreground-background separation regions using faithful guidance from dual pixels.
              </p>
            </td>
          </tr>
        </tbody>
      </table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/MonoLF.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://shrisudhang.github.io/pages/monolf-ECCV22/">
                <papertitle>Synthesizing Light Field Video from Monocular Video</papertitle>
              </a>
              <br>
              <strong>Shrisudhan Govindarajan</strong>,
              <a href="https://asprasan.github.io/">Prasan Shedligeri</a>,
              <a href="">Sarah</a>,
              <a href="https://www.ee.iitm.ac.in/kmitra/research.html">Kaushik Mitra</a>
              <br>
              <em>ECCV</em>, 2022 <strong>[oral]</strong>
              <br>
              <a href="https://shrisudhang.github.io/pages/monolf-ECCV22/">project</a>
              /
              <a href="https://arxiv.org/pdf/2207.10357">pdf</a>
              /
              <a href="https://arxiv.org/abs/2207.10357">arXiv</a>
              /
              <a href="https://github.com/ShrisudhanG/Synthesizing-Light-Field-Video-from-Monocular-Video">code</a>
              <p>
                We propose a self-supervised learning technique to reconstruct light field from monocular 
                video with following novelties: an adaptive low-rank representation for each scene, 
                an explicit disocclusion handling technique, and a novel supervised refinement block(optional)
                that exploits available ground truth Light Field image dataset.
              </p>
            </td>
          </tr>
        </tbody>
      </table>


      <!-- Invited Talks -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Invited Talks</heading>
              <p></p>
            </td>
          </tr>
        </tbody>
      </table>

      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/MIPI-logo.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://mipi-challenge.org/">
                <papertitle>Mobile Intelligent Photography and Imaging (MIPI) workshop, ECCV 2022</papertitle>
              </a>
              <br>
              <strong>Invited Talk: Synthesizing Light Field Video from Smartphones</strong>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://mipi-challenge.org/">workshop</a>
              /
              <a href="https://docs.google.com/presentation/d/1A-WHOiC7no5o6WLsOlqmjCFLGJ5jpTiKKk4rN2MwE5A/edit?usp=sharing">slides</a>
              /
              <a href="https://www.youtube.com/watch?v=2SijuyeC05w&ab_channel=MIPIWorkshop">youtube</a>
              <p></p>
              <!-- <p> 
                In the last 2 decades, we have seen a revolution in mobile imaging with improvements in both the hardware and software.
                However, these cameras capture only a 2D projection of our rich 3D world. In this talk we propose a self-supervised learning
                technique to reconstruct light field(containing 3D information) video from simple smartphone camera configurations,
                namely monocualr camera and stereo camera(2D projections). We propose various novel techniques to address the challenges associated 
                with these camera configurations in our attempt to synthesize structurally and temporally consistent light field video.
              </p> -->
            </td>
          </tr>
        </tbody>
      </table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/VisionIndia-logo.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://events.iitgn.ac.in/2022/icvgip/vision_india.html">
                <papertitle>Vision India, ICVGIP 2022</papertitle>
              </a>
              <br>
              <strong>Invited Talk: Synthesizing Light Field Video from Monocular Video</strong>
              <br>
              <em>ICVGIP</em>, 2022
              <br>
              <a href="https://events.iitgn.ac.in/2022/icvgip/vision_india.html">conference</a>
              /
              <a href="https://docs.google.com/presentation/d/17LKhPIttirrtu6fHv4mDSz5Th0xLu6khFyn9pWpuFe0/edit?usp=sharing">slides</a>
              <p></p>
              <!-- <p> 
                Learning-based techniques which solve the ill-posed problem of LF reconstruction from sparse (1, 2 or 4) views have 
                significantly reduced the requirement for complex hardware. LF video reconstruction from sparse views poses a special 
                challenge as acquiring ground-truth for training these models is hard. In this talk we propose a self-supervised learning-based
                algorithm for LF video reconstruction from monocular videos. We propose novel techniques to address the limitations of 
                monocular input sequences for light field synthesis task, like difficulty in occlusion handling and depth scale perception.
              </p> -->
            </td>
          </tr>
        </tbody>
      </table>


      <!-- Professional Experience -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Experience</heading>
              <p></p>
            </td>
          </tr>
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Microsoft-logo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.microsoft.com/en-in">
                <papertitle>Microsoft R&D, India - Search Technology Center India</papertitle>
              </a>
              <br>
              <strong>Data and Applied Scientist</strong>
              <br>
              <em>July, 2022 - August, 2023</em>
              <br>
              <!-- <p> 
                In the current version of Microsoft Teams, Office and Sharepoint space, for a given search, we see multiple entity sets,
                like People suggestions, Message suggestions, File suggestions, Calendar suggestions and others. I work on developing a 
                ranking algorithm to rank these different entity sets based on their relevance to the searched query and previous user 
                interaction.
              </p> -->
            </td>
          </tr>
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Microsoft-logo.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.microsoft.com/en-in">
                <papertitle>Microsoft R&D, India - Search Technology Center India</papertitle>
              </a>
              <br>
              <strong>Data and Applied Scientist Intern</strong>
              <br>
              <em>May, 2021 - July, 2021</em>
              <br>
              <!-- <p>
                Developed an ranking algorithm to rank related suggestions for a query based on the relatedness and usefulness of 
                the suggestion in an Enterprise-level(Microsoft Bing Work vertical) setup.
              </p>-->
            </td>
          </tr>
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>   <!-- bgcolor="#ffffd0" -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/AutoInfer-logo.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.microsoft.com/en-in">
                <papertitle>AutoInfer Pvt. Ltd.</papertitle>
              </a>
              <br>
              <strong>Deep Learning Intern</strong>
              <br>
              <em>June, 2020 - August, 2020</em>
              <br>
              <!-- <p>
                Developed a Generative Network inspired by the Layout2Image algorithm to generate realistic documents from user-specified 
                layouts. Built a table detection network inspired by LayoutLM algorithm which extracts textual and image features from the 
                document to detect tables and information.
              </p> -->
            </td>
          </tr>
        </tbody>
      </table>



      <!-- Other projects -->

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Other projects</heading>
            </td>
          </tr>
        </tbody>
      </table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Caching.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Caching in DNNs - Speeding up Inference for similar inputs</papertitle>
              </a>
              <br>
              <strong>Shrisudhan Govindarajan</strong>,
              <a href="https://scholar.google.co.in/citations?user=4pr87jAAAAAJ&hl=en">Pratyush Kumar</a>
              <br>
              <a href="data\Caching.pdf">pdf</a>
              /
              <a href="https://github.com/Shrisudhan/Shrisudhan-Caching-in-DNNs---Speeding-up-inference-for-similar-inputs">code</a>
              <p></p>
              <!-- <p>
                We propose to use caching in DNNs to improve the inference speed for classification problems and also improve the robustness
                of the network towards brightness, contrast variations, and increase immunity towards adversarial attacks.
              </p> -->
            </td>
          </tr>
        </tbody>
      </table>
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/BTP.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Battery Prognostics: Estimation of Remaining Operational Time of batteries using convolution 
                  and temporal-correlation</papertitle>
              </a>
              <!-- <br>
              <strong>Shrisudhan Govindarajan</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=tu-ntggAAAAJ">Deep Singh</a>,
              <a href="https://scholar.google.co.in/citations?user=cZBq_9MAAAAJ&hl=en">Arunachalam N</a>, -->
              <br>
              <em>Bachelors Thesis</em>
              <br>
              <a href="data\BTP_Thesis.pdf">pdf</a>
              <p></p>
              <!-- <p>
                In this study, we propose a novel deep neural network (DNN) based architecture which uses temporal information
                to learn the changes in the battery across time and utilizes that in-coherence with the available parameters 
                for estimating accurate remaining operational time.
              </p> -->
            </td>
          </tr>
        </tbody>
      </table>


      <!-- Education -->

      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
            </td>
          </tr>
        </tbody>
      </table>

      <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
        <tbody>
          <tr>
              <td style="padding:10px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/iitm_logo.png' width="110" class="side-image">
                  </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top">
                  <papertitle><big>Indian Institute of Technology Madras</big></papertitle>
                  <br>
                  <papertitle style="color:gray"><big>Integrated Dual Degree in </big></papertitle><papertitle><big>Data Science and Mechanical Engineering(Honours)</big></papertitle>
                  <br>
                  July '17 - May '22
                  <br>
              </td>
          </tr>
        </tbody>
      </table>
      </div> -->

      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
              <p>
                Participated in Intern IIT 2018, held at IIT Bombay. We presented a prototype of driver assistance system with lane 
                detection, object detection and, sign and signal detection features(<a href="https://nikotto.in/ADAS.html">Link</a>).
              </p>
              <p>
                Some of the best experiences I've had in my undergraduate life is due to Computational Imaging Group(
                  <a href="https://www.ee.iitm.ac.in/comp_photolab/">Link</a>). The seniors and people there are some of the best and 
                  loveliest you can find in IIT Madras. I am highly indebted for being a part of the group.
              </p>
              <p>
                I have also had the pleasure to be a part of the IIT Madras Wolves(football/soccer team).
              </p>
              <p>
               In my free time, I love to watch Movies, TV shows and you can most definitely find me listening to music at any time of day.
              </p>
            </td>
          </tr>
        </tbody>
      </table> -->
        
 
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                This template is stolen from <a href="https://jonbarron.info/">here</a>.
              </p>
            </td>
          </tr>
        </tbody>
      </table>


      </td>
    </tr>
  </table>
</body>

</html>